{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script we will create huge matrices with all patients and the exposome variables of interest:\n",
    "\n",
    "    1. x_nan = 115 variables with missing values (no dummy transformations) \n",
    "    2. x_s = 115 variables with all missing values imputed by simple approach (no dummy transformations) \n",
    "    3. x_s_nan = 115 variables with imputed missing values for imputation with multivariable approaches (no dummy transformations) \n",
    " \n",
    "    4. x_nan_d = 164 variables with missing values (dummy transformations)\n",
    "    5. x_s_d = 164 variables with all missing values imputed by simple approach (dummy transformations)\n",
    "    6. x_s_nan_d = 164 variables with imputed missing values for imputation with multivariable approaches (dummy transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marinacamachosanz/Desktop/tfg_BCN-AIM/all_fields\n"
     ]
    }
   ],
   "source": [
    "cd /Users/marinacamachosanz/Desktop/tfg_BCN-AIM/all_fields/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Physical_measures_Anthropometry #\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_48 = pd.read_csv(\"f.48.tab\",delimiter=\"\\t\")\n",
    "data_49 = pd.read_csv(\"f.49.tab\",delimiter=\"\\t\")\n",
    "data_50 = pd.read_csv(\"f.50.tab\",delimiter=\"\\t\")\n",
    "data_21001 = pd.read_csv(\"f.21001.tab\",delimiter=\"\\t\")\n",
    "data_21002 = pd.read_csv(\"f.21002.tab\",delimiter=\"\\t\")\n",
    "data_23099 = pd.read_csv(\"f.23099.tab\",delimiter=\"\\t\")\n",
    "data_23101 = pd.read_csv(\"f.23101.tab\",delimiter=\"\\t\")\n",
    "data_23102 = pd.read_csv(\"f.23102.tab\",delimiter=\"\\t\")\n",
    "data_23105 = pd.read_csv(\"f.23105.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_23101[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan = pd.DataFrame(data_48['f.eid'])\n",
    "x_s = pd.DataFrame(data_48['f.eid'])\n",
    "x_s_nan = pd.DataFrame(data_48['f.eid'])\n",
    "\n",
    "x_nan_d = pd.DataFrame(data_48['f.eid'])\n",
    "x_s_d = pd.DataFrame(data_48['f.eid'])\n",
    "x_s_nan_d = pd.DataFrame(data_48['f.eid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Physical_measures_Anthropometry = [data_48, data_49, data_50, data_21001, data_21002, data_23099, data_23101, data_23102, data_23105]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "col = ['waistcircum', 'hipcircum', 'standingheight', 'bmi', 'weight', 'bodyfatpercent', 'wholebodyfatfreemass', 'wholebodywatermass', 'basalmetabolicrate']\n",
    "count = -1\n",
    "for data in Physical_measures_Anthropometry:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Sociodemographics_Population_Characteristics #\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_34 = pd.read_csv(\"f.34.tab\",delimiter=\"\\t\")\n",
    "data_189 = pd.read_csv(\"f.189.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "data_34 = pd.DataFrame(data_34)\n",
    "x34 = data_34.iloc[:,1:2].values\n",
    "\n",
    "x_nan['age'] = pd.DataFrame(x34)\n",
    "x_nan_d['age'] = pd.DataFrame(x34)\n",
    "\n",
    "x_s_nan['age'] = pd.DataFrame(x34)\n",
    "x_s_nan_d['age'] = pd.DataFrame(x34)\n",
    "\n",
    "imp.fit(x34)\n",
    "x34 = imp.transform(x34)\n",
    "count=0\n",
    "for x in x34: x34[count]=int(x);count+=1\n",
    "    \n",
    "x_s['age'] = x34\n",
    "x_s_d['age'] = x34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_189 = pd.DataFrame(data_189)\n",
    "x189 = data_189.iloc[:,1:2].values\n",
    "\n",
    "x_nan['townsendeprivation'] = pd.DataFrame(x189)\n",
    "x_nan_d['townsendeprivation'] = pd.DataFrame(x189)\n",
    "\n",
    "x_s_nan['townsendeprivation'] = pd.DataFrame(x189)\n",
    "x_s_nan_d['townsendeprivation'] = pd.DataFrame(x189)\n",
    "\n",
    "imp.fit(x189)\n",
    "x189 = imp.transform(x189)\n",
    "\n",
    "x_s['townsendeprivation'] = x189\n",
    "x_s_d['townsendeprivation'] = x189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Sociodemographics_Employment #\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_757 = pd.read_csv(\"f.757.tab\",delimiter=\"\\t\")\n",
    "data_767 = pd.read_csv(\"f.767.tab\",delimiter=\"\\t\")\n",
    "data_796 = pd.read_csv(\"f.796.tab\",delimiter=\"\\t\")\n",
    "data_806 = pd.read_csv(\"f.806.tab\",delimiter=\"\\t\")\n",
    "data_816 = pd.read_csv(\"f.816.tab\",delimiter=\"\\t\")\n",
    "data_826 = pd.read_csv(\"f.826.tab\",delimiter=\"\\t\")\n",
    "data_6142 = pd.read_csv(\"f.6142.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sociodemographics_Employment_I = [data_757, data_767, data_796]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "col = ['timeinjob', 'lengthworkweek', 'distancehomework']\n",
    "count = -1\n",
    "for data in Sociodemographics_Employment_I:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3, -10, -5],[np.NaN, np.NaN, 0, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    counte = 0\n",
    "    for element in x: x[counte] = int(element); counte += 1\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sociodemographics_Employment_II = [data_806, data_816, data_826]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['jobwalkingstand', 'jobphysical', 'jobshiftwork']\n",
    "count = -1\n",
    "for data in Sociodemographics_Employment_II:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x=x.replace([-1, -3, -5], [np.NaN, np.NaN, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    counte=0\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_6142['f.6142.1.0']\n",
    "del data_6142['f.6142.1.1']\n",
    "del data_6142['f.6142.1.2']\n",
    "del data_6142['f.6142.1.3']\n",
    "del data_6142['f.6142.1.4']\n",
    "del data_6142['f.6142.1.5']\n",
    "del data_6142['f.6142.1.6']\n",
    "del data_6142['f.6142.2.0']\n",
    "del data_6142['f.6142.2.1']\n",
    "del data_6142['f.6142.2.2']\n",
    "del data_6142['f.6142.2.3']\n",
    "del data_6142['f.6142.2.4']\n",
    "del data_6142['f.6142.2.5']\n",
    "del data_6142['f.6142.2.6']\n",
    "del data_6142['f.6142.3.0']\n",
    "del data_6142['f.6142.3.1']\n",
    "del data_6142['f.6142.3.2']\n",
    "del data_6142['f.6142.3.3']\n",
    "del data_6142['f.6142.3.4']\n",
    "del data_6142['f.6142.3.5']\n",
    "del data_6142['f.6142.3.6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_6142 = data_6142.iloc[:,1:].values\n",
    "data_6142 = pd.DataFrame(data_6142)\n",
    "var = {0: 'currentemployment_0',\n",
    "       1: 'currentemployment_1',      \n",
    "       2: 'currentemployment_2',      \n",
    "       3: 'currentemployment_3',\n",
    "       4: 'currentemployment_4',\n",
    "       5: 'currentemployment_5',      \n",
    "       6: 'currentemployment_6'}\n",
    "data_6142 = data_6142.rename(columns = var)\n",
    "data_6142 = data_6142.replace(-3, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_0 = data_6142.iloc[:,0:1].values\n",
    "imp.fit(currentemployment_0)\n",
    "currentemployment_0 = imp.transform(currentemployment_0)\n",
    "currentemployment_0 = pd.DataFrame(currentemployment_0)\n",
    "dummy = pd.get_dummies(currentemployment_0[0])\n",
    "currentemployment_0 = pd.concat([currentemployment_0,dummy], axis = 1)\n",
    "del currentemployment_0[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_0 = currentemployment_0.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_1 = data_6142.iloc[:,1:2].values\n",
    "imp.fit(currentemployment_1)\n",
    "currentemployment_1 = imp.transform(currentemployment_1)\n",
    "currentemployment_1 = pd.DataFrame(currentemployment_1)\n",
    "dummy = pd.get_dummies(currentemployment_1[0])\n",
    "currentemployment_1 = pd.concat([currentemployment_1,dummy], axis = 1)\n",
    "del currentemployment_1[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_1 = currentemployment_1.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_2 = data_6142.iloc[:,2:3].values\n",
    "imp.fit(currentemployment_2)\n",
    "currentemployment_2 = imp.transform(currentemployment_2)\n",
    "currentemployment_2 = pd.DataFrame(currentemployment_2)\n",
    "dummy = pd.get_dummies(currentemployment_2[0])\n",
    "currentemployment_2 = pd.concat([currentemployment_2,dummy], axis = 1)\n",
    "del currentemployment_2[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_2 = currentemployment_2.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_3 = data_6142.iloc[:,3:4].values\n",
    "imp.fit(currentemployment_3)\n",
    "currentemployment_3 = imp.transform(currentemployment_3)\n",
    "currentemployment_3 = pd.DataFrame(currentemployment_3)\n",
    "dummy = pd.get_dummies(currentemployment_3[0])\n",
    "currentemployment_3 = pd.concat([currentemployment_3,dummy], axis = 1)\n",
    "del currentemployment_3[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_3 = currentemployment_3.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_4 = data_6142.iloc[:,4:5].values\n",
    "imp.fit(currentemployment_4)\n",
    "currentemployment_4 = imp.transform(currentemployment_4)\n",
    "currentemployment_4 = pd.DataFrame(currentemployment_4)\n",
    "dummy = pd.get_dummies(currentemployment_4[0])\n",
    "currentemployment_4 = pd.concat([currentemployment_4,dummy], axis = 1)\n",
    "del currentemployment_4[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_4 = currentemployment_4.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_5 = data_6142.iloc[:,5:6].values\n",
    "imp.fit(currentemployment_5)\n",
    "currentemployment_5 = imp.transform(currentemployment_5)\n",
    "currentemployment_5 = pd.DataFrame(currentemployment_5)\n",
    "dummy = pd.get_dummies(currentemployment_5[0])\n",
    "currentemployment_5 = pd.concat([currentemployment_5,dummy], axis = 1)\n",
    "del currentemployment_5[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_5 = currentemployment_5.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_6 = data_6142.iloc[:,6:7].values\n",
    "imp.fit(currentemployment_6)\n",
    "currentemployment_6 = imp.transform(currentemployment_6)\n",
    "currentemployment_6 = pd.DataFrame(currentemployment_6)\n",
    "dummy = pd.get_dummies(currentemployment_6[0])\n",
    "currentemployment_6 = pd.concat([currentemployment_6,dummy], axis = 1)\n",
    "del currentemployment_6[0]\n",
    "var = {-7.0: 'otherwork',\n",
    "       1.0: 'employed',      \n",
    "       2.0: 'retired',      \n",
    "       3.0: 'lookingafterhome',\n",
    "       4.0: 'unabletowork',      \n",
    "       5.0: 'unemployed',      \n",
    "       6.0: 'unpaidwork',      \n",
    "       7.0: 'student'}\n",
    "currentemployment_6 = currentemployment_6.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentemployment_f = pd.DataFrame()\n",
    "currentemployment_f['otherwork'] = currentemployment_0['otherwork'] \n",
    "currentemployment_f['employed'] = currentemployment_0['employed']       \n",
    "currentemployment_f['retired'] = currentemployment_0['retired'] + currentemployment_1['retired']    \n",
    "currentemployment_f['lookingafterhome'] = currentemployment_0['lookingafterhome'] + currentemployment_1['lookingafterhome'] + currentemployment_2['lookingafterhome']       \n",
    "currentemployment_f['unabletowork'] = currentemployment_0['unabletowork'] + currentemployment_1['unabletowork'] + currentemployment_2['unabletowork'] + currentemployment_3['unabletowork']         \n",
    "currentemployment_f['unemployed'] = currentemployment_0['unemployed'] + currentemployment_1['unemployed'] + currentemployment_2['unemployed'] + currentemployment_3['unemployed'] + currentemployment_4['unemployed']            \n",
    "currentemployment_f['unpaidwork'] = currentemployment_0['unpaidwork'] + currentemployment_1['unpaidwork'] + currentemployment_2['unpaidwork'] + currentemployment_3['unpaidwork'] + currentemployment_4['unpaidwork'] + currentemployment_5['unpaidwork']           \n",
    "currentemployment_f['student'] = currentemployment_0['student'] + currentemployment_1['student'] + currentemployment_2['student'] + currentemployment_3['student'] + currentemployment_4['student'] + currentemployment_5['student'] + currentemployment_6['student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan = pd.concat([x_nan, currentemployment_f], axis=1)\n",
    "x_nan_d = pd.concat([x_nan_d, currentemployment_f], axis=1)\n",
    "\n",
    "x_s = pd.concat([x_s, currentemployment_f], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, currentemployment_f], axis=1)\n",
    "\n",
    "x_s_nan = pd.concat([x_s_nan, currentemployment_f], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, currentemployment_f], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Sociodemographics_Education #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sociodemographics_Education\n",
    "data_6138 = pd.read_csv(\"f.6138.tab\",delimiter=\"\\t\")\n",
    "data_845 = pd.read_csv(\"f.845.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "data_845=data_845.replace(-1, np.NaN)\n",
    "data_845=data_845.replace(-3, np.NaN)\n",
    "data_845=data_845.replace(-2, 999)\n",
    "data_845 = pd.DataFrame(data_845)\n",
    "x845 = data_845.iloc[:,1:2].values\n",
    "\n",
    "x_nan['agecompleted'] = pd.DataFrame(x845)\n",
    "x_nan_d['agecompleted'] = pd.DataFrame(x845)\n",
    "\n",
    "x_s_nan['agecompleted'] = pd.DataFrame(x845)\n",
    "x_s_nan_d['agecompleted'] = pd.DataFrame(x845)\n",
    "\n",
    "imp.fit(x845)\n",
    "x845 = imp.transform(x845)\n",
    "count=0\n",
    "for x in x845: x845[count]=int(x);count+=1\n",
    "    \n",
    "x_s['agecompleted'] = x845\n",
    "x_s_d['agecompleted'] = x845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_6138['f.6138.1.0']\n",
    "del data_6138['f.6138.1.1']\n",
    "del data_6138['f.6138.1.2']\n",
    "del data_6138['f.6138.1.3']\n",
    "del data_6138['f.6138.1.4']\n",
    "del data_6138['f.6138.1.5']\n",
    "\n",
    "del data_6138['f.6138.2.0']\n",
    "del data_6138['f.6138.2.1']\n",
    "del data_6138['f.6138.2.2']\n",
    "del data_6138['f.6138.2.3']\n",
    "del data_6138['f.6138.2.4']\n",
    "del data_6138['f.6138.2.5']\n",
    "\n",
    "del data_6138['f.6138.3.0']\n",
    "del data_6138['f.6138.3.1']\n",
    "del data_6138['f.6138.3.2']\n",
    "del data_6138['f.6138.3.3']\n",
    "del data_6138['f.6138.3.4']\n",
    "del data_6138['f.6138.3.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_6138 = pd.DataFrame(data_6138)\n",
    "data_6138 = data_6138.iloc[:,1:].values\n",
    "data_6138 = pd.DataFrame(data_6138)\n",
    "var = {0: 'qualifications_0',\n",
    "       1: 'qualifications_1',      \n",
    "       2: 'qualifications_2',      \n",
    "       3: 'qualifications_3',\n",
    "       4: 'qualifications_4',\n",
    "       5: 'qualifications_5'}\n",
    "data_6138 = data_6138.rename(columns = var)\n",
    "data_6138 = data_6138.replace(-3, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_0 = data_6138.iloc[:,0:1].values\n",
    "imp.fit(qualifications_0)\n",
    "qualifications_0 = imp.transform(qualifications_0)\n",
    "qualifications_0 = pd.DataFrame(qualifications_0)\n",
    "dummy = pd.get_dummies(qualifications_0[0])\n",
    "qualifications_0 = pd.concat([qualifications_0,dummy], axis = 1)\n",
    "del qualifications_0[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_0 = qualifications_0.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_1 = data_6138.iloc[:,1:2].values\n",
    "imp.fit(qualifications_1)\n",
    "qualifications_1 = imp.transform(qualifications_1)\n",
    "qualifications_1 = pd.DataFrame(qualifications_1)\n",
    "dummy = pd.get_dummies(qualifications_1[0])\n",
    "qualifications_1 = pd.concat([qualifications_1,dummy], axis = 1)\n",
    "del qualifications_1[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_1 = qualifications_1.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_2 = data_6138.iloc[:,2:3].values\n",
    "imp.fit(qualifications_2)\n",
    "qualifications_2 = imp.transform(qualifications_2)\n",
    "qualifications_2 = pd.DataFrame(qualifications_2)\n",
    "dummy = pd.get_dummies(qualifications_2[0])\n",
    "qualifications_2 = pd.concat([qualifications_2,dummy], axis = 1)\n",
    "del qualifications_2[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_2 = qualifications_2.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_3 = data_6138.iloc[:,3:4].values\n",
    "imp.fit(qualifications_3)\n",
    "qualifications_3 = imp.transform(qualifications_3)\n",
    "qualifications_3 = pd.DataFrame(qualifications_3)\n",
    "dummy = pd.get_dummies(qualifications_3[0])\n",
    "qualifications_3 = pd.concat([qualifications_3,dummy], axis = 1)\n",
    "del qualifications_3[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_3 = qualifications_3.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_4 = data_6138.iloc[:,4:5].values\n",
    "imp.fit(qualifications_4)\n",
    "qualifications_4 = imp.transform(qualifications_4)\n",
    "qualifications_4 = pd.DataFrame(qualifications_4)\n",
    "dummy = pd.get_dummies(qualifications_4[0])\n",
    "qualifications_4 = pd.concat([qualifications_4,dummy], axis = 1)\n",
    "del qualifications_4[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_4 = qualifications_4.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_5 = data_6138.iloc[:,5:6].values\n",
    "imp.fit(qualifications_5)\n",
    "qualifications_5 = imp.transform(qualifications_5)\n",
    "qualifications_5 = pd.DataFrame(qualifications_5)\n",
    "dummy = pd.get_dummies(qualifications_5[0])\n",
    "qualifications_5 = pd.concat([qualifications_5,dummy], axis = 1)\n",
    "del qualifications_5[0]\n",
    "var = {-7.0: 'noeducation',\n",
    "       1.0: 'university',  \n",
    "       2.0: 'A/AS', \n",
    "       3.0: 'O/GCSE',\n",
    "       4.0: 'CSE',\n",
    "       5.0: 'NVQ/HND/HNC',      \n",
    "       6.0: 'professional'}\n",
    "qualifications_5 = qualifications_5.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifications_f = pd.DataFrame()\n",
    "qualifications_f['noeducation'] = qualifications_0['noeducation'] \n",
    "qualifications_f['university'] = qualifications_0['university']       \n",
    "qualifications_f['A/AS'] = qualifications_0['A/AS'] + qualifications_1['A/AS']    \n",
    "qualifications_f['O/GCSE'] = qualifications_0['O/GCSE'] + qualifications_1['O/GCSE'] + qualifications_2['O/GCSE'] \n",
    "qualifications_f['CSE'] = qualifications_0['CSE'] + qualifications_1['CSE'] + qualifications_2['CSE'] + qualifications_3['CSE']      \n",
    "qualifications_f['NVQ/HND/HNC'] = qualifications_0['NVQ/HND/HNC'] + qualifications_1['NVQ/HND/HNC'] + qualifications_2['NVQ/HND/HNC'] + qualifications_3['NVQ/HND/HNC'] + qualifications_4['NVQ/HND/HNC']\n",
    "qualifications_f['professional'] = qualifications_0['professional'] + qualifications_1['professional'] + qualifications_2['professional'] +  qualifications_3['professional'] +  qualifications_4['professional'] + qualifications_5['professional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan = pd.concat([x_nan, qualifications_f], axis=1)\n",
    "x_nan_d = pd.concat([x_nan_d, qualifications_f], axis=1)\n",
    "\n",
    "x_s = pd.concat([x_s, qualifications_f], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, qualifications_f], axis=1)\n",
    "\n",
    "x_s_nan = pd.concat([x_s_nan, qualifications_f], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, qualifications_f], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Lifestyle_and_environment_Sleep #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1160 = pd.read_csv(\"f.1160.tab\",delimiter=\"\\t\")\n",
    "data_1200 = pd.read_csv(\"f.1200.tab\",delimiter=\"\\t\")\n",
    "data_1220 = pd.read_csv(\"f.1220.tab\",delimiter=\"\\t\")\n",
    "data_1190 = pd.read_csv(\"f.1190.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1160['Unnamed: 0']\n",
    "del data_1200['Unnamed: 0']\n",
    "del data_1220['Unnamed: 0']\n",
    "del data_1190['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1160['f.1160.1.0']\n",
    "del data_1160['f.1160.2.0']\n",
    "del data_1160['f.1160.3.0']\n",
    "del data_1200['f.1200.1.0']\n",
    "del data_1200['f.1200.2.0']\n",
    "del data_1200['f.1200.3.0']\n",
    "del data_1220['f.1220.1.0']\n",
    "del data_1220['f.1220.2.0']\n",
    "del data_1220['f.1220.3.0']\n",
    "del data_1190['f.1190.1.0']\n",
    "del data_1190['f.1190.2.0']\n",
    "del data_1190['f.1190.3.0'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Sleep = [data_1200, data_1220, data_1190]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['sleeplessness', 'daytimedozing', 'napduringday']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Sleep:\n",
    "    count += 1\n",
    "    data = data.replace([-1, -3], [np.NaN, np.NaN])\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "data_1160 = pd.DataFrame(data_1160)\n",
    "x1160 = data_1160.iloc[:,1:2].values\n",
    "x1160=pd.DataFrame(x1160).replace([-1, -3],[np.NaN, np.NaN])\n",
    "\n",
    "x_nan['sleepduration'] = pd.DataFrame(x1160)\n",
    "x_nan_d['sleepduration'] = pd.DataFrame(x1160)\n",
    "\n",
    "x_s_nan['sleepduration'] = pd.DataFrame(x1160)\n",
    "x_s_nan_d['sleepduration'] = pd.DataFrame(x1160)\n",
    "\n",
    "imp.fit(x1160)\n",
    "x1160 = imp.transform(x1160)\n",
    "count=0\n",
    "for x in x1160: x1160[count]=int(x);count+=1\n",
    "\n",
    "x_s['sleepduration'] = x1160\n",
    "x_s_d['sleepduration'] = x1160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Lifestyle_and_environment_Drug_consumption #\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1239 = pd.read_csv(\"f.1239.tab\",delimiter=\"\\t\")\n",
    "data_1249 = pd.read_csv(\"f.1249.tab\",delimiter=\"\\t\")\n",
    "data_20414 = pd.read_csv(\"f.20414.tab\",delimiter=\"\\t\")\n",
    "data_20453 = pd.read_csv(\"f.20453.tab\",delimiter=\"\\t\")\n",
    "data_20411 = pd.read_csv(\"f.20411.tab\",delimiter=\"\\t\")\n",
    "data_20405 = pd.read_csv(\"f.20405.tab\",delimiter=\"\\t\")\n",
    "data_20117 = pd.read_csv(\"f.20117.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1239['Unnamed: 0']\n",
    "del data_1249['Unnamed: 0']\n",
    "del data_20117['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1239['f.1239.0.0']=data_1239['f.1239.0.0'].replace(-3, np.NaN)\n",
    "data_1239['f.1239.0.0']=data_1239['f.1239.0.0'].replace([2,1], [1,2])\n",
    "data_1249['f.1249.0.0']=data_1249['f.1249.0.0'].replace(-3, np.NaN)\n",
    "data_1249['f.1249.0.0']=data_1249['f.1249.0.0'].replace([0,1,3,4], [4,3,1,0])\n",
    "data_20414['f.20414.0.0']=data_20414['f.20414.0.0'].replace(-818, np.NaN)\n",
    "data_20453['f.20453.0.0']=data_20453['f.20453.0.0'].replace(-818, np.NaN)\n",
    "data_20411['f.20411.0.0']=data_20411['f.20411.0.0'].replace(-818, np.NaN)\n",
    "data_20405['f.20405.0.0']=data_20405['f.20405.0.0'].replace(-818, np.NaN)\n",
    "data_20117['f.20117.0.0']=data_20117['f.20117.0.0'].replace(-3, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Drug_consumption = [data_1239, data_1249, data_20414, data_20453, data_20411, data_20405, data_20117]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['currenttobacco', 'pasttobacco', 'frequdrinkingalcohol', 'evertakencannabis', 'everbeeninjuredrinkalcohol', 'everhadknownpersonalcoh', 'alcoholdrinkerstatus']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Drug_consumption:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Lifestyle_and_environment_Sun_exposure #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1050 = pd.read_csv(\"f.1050.tab\",delimiter=\"\\t\")\n",
    "data_1060 = pd.read_csv(\"f.1060.tab\",delimiter=\"\\t\")\n",
    "data_2267 = pd.read_csv(\"f.2267.tab\",delimiter=\"\\t\")\n",
    "data_1747 = pd.read_csv(\"f.1747.tab\",delimiter=\"\\t\")\n",
    "data_1757 = pd.read_csv(\"f.1757.tab\",delimiter=\"\\t\")\n",
    "data_1727 = pd.read_csv(\"f.1727.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1050['Unnamed: 0']\n",
    "del data_1060['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Sun_exposure_I = [data_2267, data_1757, data_1727 ]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['usesunprotection', 'facialageing', 'easeofskin']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Sun_exposure_I:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3, -10], [np.NaN, np.NaN, 0])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Sun_exposure_II = [data_1050, data_1060]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "col = ['timespendsummer', 'timespendswinter']\n",
    "count =- 1\n",
    "for data in Lifestyle_and_environment_Sun_exposure_II:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3, -10], [np.NaN, np.NaN, 0])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    counte=0\n",
    "    for element in x: x[counte]=int(element);counte+=1\n",
    "        \n",
    "    x_s[col[count]] = pd.DataFrame(x)        \n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1747['f.1747.0.0']=data_1747['f.1747.0.0'].replace(-1, np.NaN)\n",
    "data_1747['f.1747.0.0']=data_1747['f.1747.0.0'].replace(-3, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1747['f.1747.1.0']\n",
    "del data_1747['f.1747.2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan['hair'] =  pd.DataFrame(data_1747['f.1747.0.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "data_1747 = pd.DataFrame(data_1747)\n",
    "data_1747 = pd.DataFrame(data_1747.iloc[:,1:2].values)\n",
    "imp.fit(data_1747)\n",
    "data_1747 = imp.transform(data_1747)\n",
    "data_1747 = pd.DataFrame(data_1747)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s['hair'] =  data_1747[0]\n",
    "x_s_nan['hair'] =  data_1747[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(data_1747[0])\n",
    "data_1747 = pd.concat([data_1747,dummy], axis = 1)\n",
    "del data_1747[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_temp = {1.0: 'blonde',\n",
    "            2.0: 'red',      \n",
    "            3.0: 'light_brown',      \n",
    "            4.0: 'dark_brown',\n",
    "            5.0: 'black',      \n",
    "            6.0: 'otherhair'}     \n",
    "data_1747 = data_1747.rename(columns = var_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan_d = pd.concat([x_nan_d, data_1747], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, data_1747], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, data_1747], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Lifestyle_and_environment_Electronic_device_use #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1110 = pd.read_csv(\"f.1110.tab\",delimiter=\"\\t\")\n",
    "data_1120 = pd.read_csv(\"f.1120.tab\",delimiter=\"\\t\")\n",
    "data_1140 = pd.read_csv(\"f.1140.tab\",delimiter=\"\\t\")\n",
    "data_1150 = pd.read_csv(\"f.1150.tab\",delimiter=\"\\t\")\n",
    "data_2237 = pd.read_csv(\"f.2237.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1110['Unnamed: 0']\n",
    "del data_1120['Unnamed: 0']\n",
    "del data_1140['Unnamed: 0']\n",
    "del data_1140['f.1140.1.0']\n",
    "del data_1150['Unnamed: 0']\n",
    "del data_1150['f.1150.3.0']\n",
    "del data_1150['f.1150.2.0']\n",
    "del data_1150['f.1150.1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Electronic_device_use = [data_1110, data_1120, data_1140, data_2237]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['lengthmobileuse', 'weeklyusemobile', 'differencemobile2years', 'playscomputergames']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Electronic_device_use:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3],[np.NaN, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1150['f.1150.0.0'] = pd.DataFrame(data_1150['f.1150.0.0'].replace([-1, -3], [np.NaN, np.NaN ]))\n",
    "x_nan['mobilhead'] = data_1150['f.1150.0.0']\n",
    "data_1150 = pd.DataFrame(data_1150.iloc[:,1:2].values)\n",
    "imp.fit(data_1150)\n",
    "data_1150 = imp.transform(data_1150)\n",
    "data_1150 = pd.DataFrame(data_1150)\n",
    "\n",
    "x_s['mobilhead'] = data_1150[0]\n",
    "x_s_nan['mobilhead'] = data_1150[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1150 = pd.get_dummies(data_1150[0])\n",
    "data_1150 = pd.concat([data_1150,dummy1150], axis = 1)\n",
    "data_1150.drop([0], axis = 1)\n",
    "del data_1150[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1150 = {1.0: 'left',\n",
    "            2.0: 'right',      \n",
    "            3.0: 'leftright'}     \n",
    "data_1150 = data_1150.rename(columns = var_1150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan_d = pd.concat([x_nan_d, data_1150], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, data_1150], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, data_1150], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Lifestyle_and_environment_Diet #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1289 = pd.read_csv(\"f.1289.tab\",delimiter=\"\\t\")\n",
    "data_1299 = pd.read_csv(\"f.1299.tab\",delimiter=\"\\t\")\n",
    "data_1309 = pd.read_csv(\"f.1309.tab\",delimiter=\"\\t\")\n",
    "data_1319 = pd.read_csv(\"f.1319.tab\",delimiter=\"\\t\")\n",
    "data_1329 = pd.read_csv(\"f.1329.tab\",delimiter=\"\\t\")\n",
    "data_1339 = pd.read_csv(\"f.1339.tab\",delimiter=\"\\t\")\n",
    "data_1349 = pd.read_csv(\"f.1349.tab\",delimiter=\"\\t\")\n",
    "data_1359 = pd.read_csv(\"f.1359.tab\",delimiter=\"\\t\")\n",
    "data_1369 = pd.read_csv(\"f.1369.tab\",delimiter=\"\\t\")\n",
    "data_1379 = pd.read_csv(\"f.1379.tab\",delimiter=\"\\t\")\n",
    "data_1389 = pd.read_csv(\"f.1389.tab\",delimiter=\"\\t\")\n",
    "data_1408 = pd.read_csv(\"f.1408.tab\",delimiter=\"\\t\")\n",
    "data_1418 = pd.read_csv(\"f.1418.tab\",delimiter=\"\\t\")\n",
    "data_1438 = pd.read_csv(\"f.1438.tab\",delimiter=\"\\t\")\n",
    "data_1448 = pd.read_csv(\"f.1448.tab\",delimiter=\"\\t\")\n",
    "data_1458 = pd.read_csv(\"f.1458.tab\",delimiter=\"\\t\")\n",
    "data_1468 = pd.read_csv(\"f.1468.tab\",delimiter=\"\\t\")\n",
    "data_1478 = pd.read_csv(\"f.1478.tab\",delimiter=\"\\t\")\n",
    "data_1488 = pd.read_csv(\"f.1488.tab\",delimiter=\"\\t\")\n",
    "data_1498 = pd.read_csv(\"f.1498.tab\",delimiter=\"\\t\")\n",
    "data_1508 = pd.read_csv(\"f.1508.tab\",delimiter=\"\\t\")\n",
    "data_1548 = pd.read_csv(\"f.1548.tab\",delimiter=\"\\t\")\n",
    "data_1428 = pd.read_csv(\"f.1428.tab\",delimiter=\"\\t\")\n",
    "data_1528 = pd.read_csv(\"f.1528.tab\",delimiter=\"\\t\")\n",
    "data_1538 = pd.read_csv(\"f.1538.tab\",delimiter=\"\\t\") \n",
    "data_2654 = pd.read_csv(\"f.2654.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_1289['Unnamed: 0']\n",
    "del data_1289['f.1289.3.0']\n",
    "del data_1289['f.1289.1.0']\n",
    "del data_1289['f.1289.2.0']\n",
    "\n",
    "del data_1299['Unnamed: 0']\n",
    "del data_1299['f.1299.3.0']\n",
    "del data_1299['f.1299.1.0']\n",
    "del data_1299['f.1299.2.0']\n",
    "\n",
    "del data_1309['Unnamed: 0']\n",
    "del data_1309['f.1309.3.0']\n",
    "del data_1309['f.1309.1.0']\n",
    "del data_1309['f.1309.2.0']\n",
    "\n",
    "del data_1319['Unnamed: 0']\n",
    "del data_1319['f.1319.3.0']\n",
    "del data_1319['f.1319.1.0']\n",
    "del data_1319['f.1319.2.0']\n",
    "\n",
    "del data_1329['f.1329.3.0']\n",
    "del data_1329['f.1329.1.0']\n",
    "del data_1329['f.1329.2.0']\n",
    "\n",
    "del data_1339['f.1339.3.0']\n",
    "del data_1339['f.1339.1.0']\n",
    "del data_1339['f.1339.2.0']\n",
    "\n",
    "del data_1349['f.1349.3.0']\n",
    "del data_1349['f.1349.1.0']\n",
    "del data_1349['f.1349.2.0']\n",
    "\n",
    "del data_1359['f.1359.3.0']\n",
    "del data_1359['f.1359.1.0']\n",
    "del data_1359['f.1359.2.0']\n",
    "\n",
    "del data_1369['f.1369.3.0']\n",
    "del data_1369['f.1369.1.0']\n",
    "del data_1369['f.1369.2.0']\n",
    "\n",
    "del data_1379['f.1379.3.0']\n",
    "del data_1379['f.1379.1.0']\n",
    "del data_1379['f.1379.2.0']\n",
    "\n",
    "del data_1389['f.1389.3.0']\n",
    "del data_1389['f.1389.1.0']\n",
    "del data_1389['f.1389.2.0']\n",
    "\n",
    "del data_1408['f.1408.3.0']\n",
    "del data_1408['f.1408.1.0']\n",
    "del data_1408['f.1408.2.0']\n",
    "\n",
    "del data_1418['f.1418.3.0']\n",
    "del data_1418['f.1418.1.0']\n",
    "del data_1418['f.1418.2.0']\n",
    "\n",
    "del data_1438['f.1438.3.0']\n",
    "del data_1438['f.1438.1.0']\n",
    "del data_1438['f.1438.2.0']\n",
    "\n",
    "del data_1448['f.1448.3.0']\n",
    "del data_1448['f.1448.1.0']\n",
    "del data_1448['f.1448.2.0']\n",
    "\n",
    "del data_1458['f.1458.1.0']\n",
    "del data_1458['f.1458.2.0']\n",
    "del data_1458['f.1458.3.0']\n",
    "\n",
    "del data_1468['f.1468.3.0']\n",
    "del data_1468['f.1468.1.0']\n",
    "del data_1468['f.1468.2.0']\n",
    "\n",
    "del data_1478['f.1478.3.0']\n",
    "del data_1478['f.1478.1.0']\n",
    "del data_1478['f.1478.2.0']\n",
    "\n",
    "del data_1488['f.1488.3.0']\n",
    "del data_1488['f.1488.1.0']\n",
    "del data_1488['f.1488.2.0']\n",
    "\n",
    "del data_1498['f.1498.3.0']\n",
    "del data_1498['f.1498.1.0']\n",
    "del data_1498['f.1498.2.0']\n",
    "\n",
    "del data_1508['f.1508.3.0']\n",
    "del data_1508['f.1508.1.0']\n",
    "del data_1508['f.1508.2.0']\n",
    "\n",
    "del data_1548['f.1548.3.0']\n",
    "del data_1548['f.1548.1.0']\n",
    "del data_1548['f.1548.2.0']\n",
    "\n",
    "del data_1428['f.1428.3.0']\n",
    "del data_1428['f.1428.1.0']\n",
    "del data_1428['f.1428.2.0']\n",
    "\n",
    "del data_1528['f.1528.3.0']\n",
    "del data_1528['f.1528.1.0']\n",
    "del data_1528['f.1528.2.0']\n",
    "\n",
    "del data_1538['f.1538.3.0']\n",
    "del data_1538['f.1538.1.0']\n",
    "del data_1538['f.1538.2.0']\n",
    "\n",
    "del data_2654['f.2654.3.0']\n",
    "del data_2654['f.2654.1.0']\n",
    "del data_2654['f.2654.2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Diet_I = [data_1289, data_1299, data_1309, data_1319, data_1438, data_1458, data_1488, data_1498, data_1528]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "col = ['cookedvegetable', 'saladintake', 'fruitintake', 'driedfruit', 'breadintake' ,'cerealintake', 'teaintake', 'coffeintake', 'waterintake']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Diet_I:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3, -10], [np.NaN, np.NaN, 0])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    counte=0\n",
    "    for element in x: x[counte]=int(element);counte+=1\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)   \n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Diet_II = [data_1329, data_1339, data_1349, data_1359, data_1369, data_1379, data_1389, data_1408, data_1418, data_1478, data_1548]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['oilyfishintake', 'nonoilyfish', 'processedmeat', 'poultryintake', 'beefintake', 'lambintake', 'porkintake', 'cheeseintake', 'milkused', 'salttofood', 'variationdiet']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Diet_II:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3],[np.NaN, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Diet_III = [data_1448, data_1468, data_1428, data_1538, data_2654, data_1508]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['breadtype','cerealtype','spreadtype','dietarychange','nonbutterspread','coffetype',]\n",
    "count = -1\n",
    "xdum = pd.DataFrame(data_1448['f.eid'])\n",
    "for data in Lifestyle_and_environment_Diet_III:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3], [np.NaN, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    \n",
    "    x = imp.transform(x)\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    xdum[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "del xdum['f.eid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['breadtype'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['breadtype']\n",
    "var = {1.0: 'white',\n",
    "       2.0: 'brown',      \n",
    "       3.0: 'wholemealgrain',\n",
    "       4.0: 'otherbread'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['cerealtype'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['cerealtype']\n",
    "var = {1.0: 'brancereal',\n",
    "       2.0: 'biscuitcereal',      \n",
    "       3.0: 'oatcereal',\n",
    "       4.0: 'muesli',\n",
    "       5.0: 'othercereal'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['spreadtype'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['spreadtype']\n",
    "var = {0.0: 'nospread',\n",
    "       1.0: 'butter',      \n",
    "       2.0: 'florabenecol_s',\n",
    "       3.0: 'otherpread_s'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['dietarychange'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['dietarychange']\n",
    "var = {0.0: 'nodietchange',\n",
    "       1.0: 'yesilldietchange',      \n",
    "       2.0: 'yesotherdietchange'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['nonbutterspread'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['nonbutterspread']\n",
    "var = {4.0: 'softmargarine',\n",
    "       5.0: 'hardmargarine',      \n",
    "       6.0: 'olive',\n",
    "       7.0: 'sunflowerolive',\n",
    "       2.0: 'florabenecol_n',\n",
    "       8.0: 'otherlowspread',\n",
    "       9.0: 'otherspread_n'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(xdum['coffetype'])\n",
    "xdum = pd.concat([xdum,dummy], axis = 1)\n",
    "del xdum['coffetype']\n",
    "var = {1:'decaffeinated',\n",
    "       2:'instantcoffee',\n",
    "       3:'groundcoffee',\n",
    "       4:'othercoffee'}\n",
    "xdum = xdum.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan_d = pd.concat([x_nan_d, xdum], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, xdum], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, xdum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Lifestyle_and_environment_Diet_by_24_hour_recall #\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_100240 = pd.read_csv(\"f.100240.tab\",delimiter=\"\\t\")\n",
    "data_100390 = pd.read_csv(\"f.100390.tab\",delimiter=\"\\t\")\n",
    "data_100580 = pd.read_csv(\"f.100580.tab\",delimiter=\"\\t\")\n",
    "data_104670 = pd.read_csv(\"f.104670.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_100240['f.100240.3.0']\n",
    "del data_100240['f.100240.1.0']\n",
    "del data_100240['f.100240.2.0']\n",
    "del data_100240['f.100240.4.0']\n",
    "del data_100240['Unnamed: 0']\n",
    "del data_100390['f.100390.3.0']\n",
    "del data_100390['f.100390.1.0']\n",
    "del data_100390['f.100390.2.0']\n",
    "del data_100390['f.100390.4.0']\n",
    "del data_100390['Unnamed: 0']\n",
    "del data_100580['f.100580.3.0']\n",
    "del data_100580['f.100580.1.0']\n",
    "del data_100580['f.100580.2.0']\n",
    "del data_100580['f.100580.4.0']\n",
    "del data_100580['Unnamed: 0']\n",
    "del data_104670['f.104670.3.0']\n",
    "del data_104670['f.104670.1.0']\n",
    "del data_104670['f.104670.2.0']\n",
    "del data_104670['f.104670.4.0']\n",
    "del data_104670['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lifestyle_and_environment_Diet_by_24_hour_recall = [data_100240, data_100390, data_100580, data_104670]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['coffeeconsumed', 'teaconsumed', 'alcoholconsumed', 'vitaminsupplementuser']\n",
    "count = -1\n",
    "for data in Lifestyle_and_environment_Diet_by_24_hour_recall:\n",
    "    count += 1\n",
    "    x = pd.DataFrame(data.iloc[:,1:2].values)\n",
    "    x = x.replace([-1, -3, -10], [np.NaN, np.NaN, 0])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "\n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "\n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Environmental_factors_Residential_air_pollution #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_24009 = pd.read_csv(\"f.24009.tab\",delimiter=\"\\t\")\n",
    "data_24014 = pd.read_csv(\"f.24014.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "x24009 = data_24009.iloc[:,1:2].values\n",
    "\n",
    "x_nan['trafficintensity'] = pd.DataFrame(x24009)\n",
    "x_nan_d['trafficintensity'] = pd.DataFrame(x24009)\n",
    "\n",
    "x_s_nan['trafficintensity'] = pd.DataFrame(x24009)\n",
    "x_s_nan_d['trafficintensity'] = pd.DataFrame(x24009)\n",
    "\n",
    "\n",
    "imp.fit(x24009)\n",
    "x24009 = imp.transform(x24009)\n",
    "count=0\n",
    "for x in x24009: x24009[count]=int(x);count+=1\n",
    "    \n",
    "x_s['trafficintensity'] = pd.DataFrame(x24009)\n",
    "x_s_d['trafficintensity'] = pd.DataFrame(x24009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x104670 = data_104670.iloc[:,1:2].values\n",
    "\n",
    "x_nan['closetomajorroad'] = pd.DataFrame(x104670)\n",
    "x_nan_d['closetomajorroad'] = pd.DataFrame(x104670)\n",
    "\n",
    "imp.fit(x104670)\n",
    "x104670 = imp.transform(x104670)\n",
    "\n",
    "x_s_nan['closetomajorroad'] = pd.DataFrame(x104670)\n",
    "x_s_nan_d['closetomajorroad'] = pd.DataFrame(x104670)\n",
    "\n",
    "x_s['closetomajorroad'] = pd.DataFrame(x104670)\n",
    "x_s_d['closetomajorroad'] = pd.DataFrame(x104670)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Environmental_factors_Residential_noise_pollution #\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_24020 = pd.read_csv(\"f.24020.tab\",delimiter=\"\\t\")\n",
    "data_24021 = pd.read_csv(\"f.24021.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "x24020 = data_24020.iloc[:,1:2].values\n",
    "\n",
    "x_nan['averagedaytimenoisepoll'] = pd.DataFrame(x24020)\n",
    "x_nan_d['averagedaytimenoisepoll'] = pd.DataFrame(x24020)\n",
    "\n",
    "x_s_nan['averagedaytimenoisepoll'] = pd.DataFrame(x24020)\n",
    "x_s_nan_d['averagedaytimenoisepoll'] = pd.DataFrame(x24020)\n",
    "\n",
    "imp.fit(x24020)\n",
    "x24020 = imp.transform(x24020)\n",
    "\n",
    "x_s['averagedaytimenoisepoll'] = pd.DataFrame(x24020)\n",
    "x_s_d['averagedaytimenoisepoll'] = pd.DataFrame(x24020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "x24021 = data_24021.iloc[:,1:2].values\n",
    "\n",
    "x_nan['averageeveningnoise'] = pd.DataFrame(x24021)\n",
    "x_nan_d['averageeveningnoise'] = pd.DataFrame(x24021)\n",
    "\n",
    "x_s_nan['averageeveningnoise'] = pd.DataFrame(x24021)\n",
    "x_s_nan_d['averageeveningnoise'] = pd.DataFrame(x24021)\n",
    "\n",
    "imp.fit(x24021)\n",
    "x24021 = imp.transform(x24021)\n",
    "\n",
    "x_s['averageeveningnoise'] = pd.DataFrame(x24021)\n",
    "x_s_d['averageeveningnoise'] = pd.DataFrame(x24021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Early_life_factors_Sun_exposure  #\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1737 = pd.read_csv(\"f.1737.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1737['f.1737.0.0']=data_1737['f.1737.0.0'].replace(-1, np.NaN)\n",
    "data_1737['f.1737.0.0']=data_1737['f.1737.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "x1737 = data_1737.iloc[:,1:2].values\n",
    "\n",
    "x_nan['childhoodsunburn'] = pd.DataFrame(x1737)\n",
    "x_nan_d['childhoodsunburn'] = pd.DataFrame(x1737)\n",
    "\n",
    "x_s_nan['childhoodsunburn'] = pd.DataFrame(x1737)\n",
    "x_s_nan_d['childhoodsunburn'] = pd.DataFrame(x1737)\n",
    "\n",
    "imp.fit(x1737)\n",
    "x1737 = imp.transform(x1737)\n",
    "count=0\n",
    "for x in x1737: x1737[count]=int(x);count+=1\n",
    "\n",
    "x_s['childhoodsunburn'] = pd.DataFrame(x1737)\n",
    "x_s_d['childhoodsunburn'] = pd.DataFrame(x1737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Early_life_factors_Early_life_factors  #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1677 = pd.read_csv(\"f.1677.tab\",delimiter=\"\\t\")\n",
    "data_1687 = pd.read_csv(\"f.1687.tab\",delimiter=\"\\t\")\n",
    "data_1697 = pd.read_csv(\"f.1697.tab\",delimiter=\"\\t\")\n",
    "data_1707 = pd.read_csv(\"f.1707.tab\",delimiter=\"\\t\")\n",
    "data_1767 = pd.read_csv(\"f.1767.tab\",delimiter=\"\\t\")\n",
    "data_1777 = pd.read_csv(\"f.1777.tab\",delimiter=\"\\t\")\n",
    "data_1787 = pd.read_csv(\"f.1787.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1677['f.1677.0.0'] = data_1677['f.1677.0.0'].replace(-1, np.NaN)\n",
    "data_1677['f.1677.0.0'] = data_1677['f.1677.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1677 = data_1677.iloc[:,1:2].values\n",
    "\n",
    "x_nan['breastfedbaby'] = pd.DataFrame(x1677)\n",
    "x_nan_d['breastfedbaby'] = pd.DataFrame(x1677)\n",
    "\n",
    "imp.fit(x1677)\n",
    "x1677 = imp.transform(x1677)\n",
    "\n",
    "x_s_nan['breastfedbaby'] = pd.DataFrame(x1677)\n",
    "x_s_nan_d['breastfedbaby'] = pd.DataFrame(x1677)\n",
    "\n",
    "x_s['breastfedbaby'] = pd.DataFrame(x1677)\n",
    "x_s_d['breastfedbaby'] = pd.DataFrame(x1677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1687['f.1687.0.0'] = data_1687['f.1687.0.0'].replace(-1, np.NaN)\n",
    "data_1687['f.1687.0.0'] = data_1687['f.1687.0.0'].replace(-3, np.NaN)\n",
    "data_1687['f.1687.0.0'] = data_1687['f.1687.0.0'].replace([2,3], [3,2])\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1687 = data_1687.iloc[:,1:2].values\n",
    "\n",
    "x_nan['bodysizeat10'] = pd.DataFrame(x1687)\n",
    "x_nan_d['bodysizeat10'] = pd.DataFrame(x1687)\n",
    "\n",
    "imp.fit(x1687)\n",
    "x1687 = imp.transform(x1687)\n",
    "\n",
    "x_s_nan['bodysizeat10'] = pd.DataFrame(x1687)\n",
    "x_s_nan_d['bodysizeat10'] = pd.DataFrame(x1687)\n",
    "\n",
    "x_s['bodysizeat10'] = pd.DataFrame(x1687)\n",
    "x_s_d['bodysizeat10'] = pd.DataFrame(x1687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1697['f.1697.0.0'] = data_1697['f.1697.0.0'].replace(-1, np.NaN)\n",
    "data_1697['f.1697.0.0'] = data_1697['f.1697.0.0'].replace(-3, np.NaN)\n",
    "data_1697['f.1697.0.0'] = data_1697['f.1697.0.0'].replace([2,3], [3,2])\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1697 = data_1697.iloc[:,1:2].values\n",
    "\n",
    "x_nan['bodyheightat10'] = pd.DataFrame(x1697)\n",
    "x_nan_d['bodyheightat10'] = pd.DataFrame(x1697)\n",
    "\n",
    "imp.fit(x1697)\n",
    "x1697 = imp.transform(x1697)\n",
    "\n",
    "x_s['bodyheightat10'] = pd.DataFrame(x1697)\n",
    "x_s_d['bodyheightat10'] = pd.DataFrame(x1697)\n",
    "\n",
    "x_s_nan['bodyheightat10'] = pd.DataFrame(x1697)\n",
    "x_s_nan_d['bodyheightat10'] = pd.DataFrame(x1697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1767['f.1767.0.0']=data_1767['f.1767.0.0'].replace(-1, np.NaN)\n",
    "data_1767['f.1767.0.0']=data_1767['f.1767.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1767 = data_1767.iloc[:,1:2].values\n",
    "\n",
    "x_nan['adoptedaschild'] = pd.DataFrame(x1767)\n",
    "x_nan_d['adoptedaschild'] = pd.DataFrame(x1767)\n",
    "\n",
    "imp.fit(x1767)\n",
    "x1767 = imp.transform(x1767)\n",
    "\n",
    "x_s['adoptedaschild'] = pd.DataFrame(x1767)\n",
    "x_s_d['adoptedaschild'] = pd.DataFrame(x1767)\n",
    "\n",
    "x_s_nan['adoptedaschild'] = pd.DataFrame(x1767)\n",
    "x_s_nan_d['adoptedaschild'] = pd.DataFrame(x1767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1777['f.1777.0.0'] = data_1777['f.1777.0.0'].replace(-1, np.NaN)\n",
    "data_1777['f.1777.0.0'] = data_1777['f.1777.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1777 = data_1777.iloc[:,1:2].values\n",
    "\n",
    "x_nan['partmultiplebirth'] = pd.DataFrame(x1777)\n",
    "x_nan_d['partmultiplebirth'] = pd.DataFrame(x1777)\n",
    "\n",
    "imp.fit(x1777)\n",
    "x1777 = imp.transform(x1777)\n",
    "\n",
    "x_s['partmultiplebirth'] = pd.DataFrame(x1777)\n",
    "x_s_d['partmultiplebirth'] = pd.DataFrame(x1777)\n",
    "\n",
    "x_s_nan['partmultiplebirth'] = pd.DataFrame(x1777)\n",
    "x_s_nan_d['partmultiplebirth'] = pd.DataFrame(x1777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1787['f.1787.0.0'] = data_1787['f.1787.0.0'].replace(-1, np.NaN)\n",
    "data_1787['f.1787.0.0'] = data_1787['f.1787.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1787 = data_1787.iloc[:,1:2].values\n",
    "\n",
    "x_nan['maternalsmoking'] = pd.DataFrame(x1787)\n",
    "x_nan_d['maternalsmoking'] = pd.DataFrame(x1787)\n",
    "\n",
    "imp.fit(x1787)\n",
    "x1787 = imp.transform(x1787)\n",
    "\n",
    "x_s['maternalsmoking'] = pd.DataFrame(x1787)\n",
    "x_s_d['maternalsmoking'] = pd.DataFrame(x1787)\n",
    "\n",
    "x_s_nan['maternalsmoking'] = pd.DataFrame(x1787)\n",
    "x_s_nan_d['maternalsmoking'] = pd.DataFrame(x1787)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1707['f.1707.0.0'] = data_1707['f.1707.0.0'].replace(-3, np.NaN)\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x1707 = data_1707.iloc[:,1:2].values\n",
    "x_nan['handedness'] = pd.DataFrame(x1707)\n",
    "\n",
    "imp.fit(x1707)\n",
    "x1707 = imp.transform(x1707)\n",
    "x1707 = pd.DataFrame(x1707)\n",
    "\n",
    "x_s['handedness'] = pd.DataFrame(x1707[0])\n",
    "x_s_nan['handedness'] = pd.DataFrame(x1707[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1707 = pd.get_dummies(x1707[0])\n",
    "x1707 = pd.concat([x1707,dummy1707], axis = 1)\n",
    "del x1707[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x1707 = {1.0: 'righthand',\n",
    "             2.0: 'lefthand',     \n",
    "             3.0: 'rightlefthand'}     \n",
    "x1707 = x1707.rename(columns = var_x1707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan_d = pd.concat([x_nan_d, x1707], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, x1707], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, x1707], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Early_life_factors_Traumatic_events #\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20491 = pd.read_csv(\"f.20491.tab\",delimiter=\"\\t\")\n",
    "data_20490 = pd.read_csv(\"f.20490.tab\",delimiter=\"\\t\")\n",
    "data_20489 = pd.read_csv(\"f.20489.tab\",delimiter=\"\\t\")\n",
    "data_20488 = pd.read_csv(\"f.20488.tab\",delimiter=\"\\t\")\n",
    "data_20487 = pd.read_csv(\"f.20487.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Early_life_factors_Traumatic_events = [data_20491, data_20490, data_20489, data_20488, data_20487]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['someonetakedoctorchild', 'sexuallymolestedchild', 'feltlovedchild', 'physicallyabusedfamiliychild', 'felthatedfamiliychild']\n",
    "count = -1\n",
    "for data in Early_life_factors_Traumatic_events:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x = pd.DataFrame(x).replace(-818, np.NaN)\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Traumatic_events_Traumatic_events #\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20531 = pd.read_csv(\"f.20531.tab\",delimiter=\"\\t\")\n",
    "data_20530 = pd.read_csv(\"f.20530.tab\",delimiter=\"\\t\")\n",
    "data_20529 = pd.read_csv(\"f.20529.tab\",delimiter=\"\\t\")\n",
    "data_20526 = pd.read_csv(\"f.20526.tab\",delimiter=\"\\t\")\n",
    "data_20525 = pd.read_csv(\"f.20525.tab\",delimiter=\"\\t\")\n",
    "data_20524 = pd.read_csv(\"f.20524.tab\",delimiter=\"\\t\")\n",
    "data_20523 = pd.read_csv(\"f.20523.tab\",delimiter=\"\\t\")\n",
    "data_20522 = pd.read_csv(\"f.20522.tab\",delimiter=\"\\t\")\n",
    "data_20521 = pd.read_csv(\"f.20521.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Early_life_factors_Traumatic_events = [data_20531, data_20530, data_20529, data_20526, data_20525, data_20524, data_20523, data_20522, data_20521]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['victimofsexualassault', 'witnessedsuddenviolentdeath', 'victimphysicallyviolentcrime', 'beenseriousaccidentlifethrea', 'abletopaymortgageadult', 'sexualinterferenceadult', 'physicalviolenceadult', 'beenconfidingrelationadult', 'belittlementbypartneradult']\n",
    "count = -1\n",
    "for data in Early_life_factors_Traumatic_events:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x = pd.DataFrame(x).replace(-818, np.NaN)\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Mental_health_Traumatic_events #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20498 = pd.read_csv(\"f.20498.tab\",delimiter=\"\\t\")\n",
    "data_20497 = pd.read_csv(\"f.20497.tab\",delimiter=\"\\t\")\n",
    "data_20495 = pd.read_csv(\"f.20495.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mental_health_Traumatic_events = [data_20530, data_20529, data_20526]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['upsetstressfulpastmonth', 'repeatedisturbingpastmonth', 'avoidstresspasthmonth']\n",
    "count = -1\n",
    "for data in Mental_health_Traumatic_events:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x = pd.DataFrame(x).replace(-818, np.NaN)\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Mental_health_Psychosocial_factors #\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2040 = pd.read_csv(\"f.2040.tab\",delimiter=\"\\t\")\n",
    "data_2050 = pd.read_csv(\"f.2050.tab\",delimiter=\"\\t\")\n",
    "data_2090 = pd.read_csv(\"f.2090.tab\",delimiter=\"\\t\")\n",
    "data_2100= pd.read_csv(\"f.2100.tab\",delimiter=\"\\t\")\n",
    "data_4526 = pd.read_csv(\"f.4526.tab\",delimiter=\"\\t\")\n",
    "data_4598 = pd.read_csv(\"f.4598.tab\",delimiter=\"\\t\")\n",
    "data_4609 = pd.read_csv(\"f.4609.tab\",delimiter=\"\\t\")\n",
    "data_4620 = pd.read_csv(\"f.4620.tab\",delimiter=\"\\t\")\n",
    "data_20126 = pd.read_csv(\"f.20126.tab\",delimiter=\"\\t\")\n",
    "data_20127 = pd.read_csv(\"f.20127.tab\",delimiter=\"\\t\")\n",
    "data_4631 = pd.read_csv(\"f.4631.tab\",delimiter=\"\\t\")\n",
    "data_2080 = pd.read_csv(\"f.2080.tab\",delimiter=\"\\t\")\n",
    "data_2070 = pd.read_csv(\"f.2070.tab\",delimiter=\"\\t\")\n",
    "data_2060 = pd.read_csv(\"f.2060.tab\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mental_health_Psychosocial_factors = [data_2040, data_2050, data_2090, data_2100, data_4526, data_4598, data_4609, data_4620, data_4631, data_2080, data_2070, data_2060]\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "col = ['risktaking', 'freqdepressed', 'seendoctordepress', 'seenpsychiatrist', 'happiness', 'everdepressed', 'perioddepress', 'numberdepressper', 'everdesinteresedweek', 'freqtiredness2weeks', 'freqtensenness2weeks', 'frequnenthusiasm2weeks']\n",
    "count = -1\n",
    "for data in Mental_health_Psychosocial_factors:\n",
    "    count += 1\n",
    "    x = data.iloc[:,1:2].values\n",
    "    x = pd.DataFrame(x).replace([-818,-1], [np.NaN, np.NaN])\n",
    "    \n",
    "    x_nan[col[count]] = pd.DataFrame(x)\n",
    "    x_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    imp.fit(x)\n",
    "    x = imp.transform(x)\n",
    "    \n",
    "    x_s_nan_d[col[count]] = pd.DataFrame(x)\n",
    "    x_s_nan[col[count]] = pd.DataFrame(x)\n",
    "    \n",
    "    x_s[col[count]] = pd.DataFrame(x)\n",
    "    x_s_d[col[count]] = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "data_20127 = pd.DataFrame(data_20127)\n",
    "x20127 = data_20127.iloc[:,1:2].values\n",
    "x20127 = pd.DataFrame(x20127).replace([-818,-1], [np.NaN, np.NaN])\n",
    "\n",
    "x_nan['neuroticismscore'] = x20127\n",
    "x_nan_d['neuroticismscore'] = x20127\n",
    "\n",
    "x_s_nan['neuroticismscore'] = x20127\n",
    "x_s_nan_d['neuroticismscore'] = x20127\n",
    "\n",
    "imp.fit(x20127)\n",
    "x20127 = imp.transform(x20127)\n",
    "count=0\n",
    "for x in x20127: x20127[count]=int(x);count+=1\n",
    "\n",
    "x_s['neuroticismscore'] = x20127\n",
    "x_s_d['neuroticismscore'] = x20127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\n",
    "x20126 = data_20126.iloc[:,1:2].values\n",
    "x20126 = pd.DataFrame(x20126).replace([-818,-1], [np.NaN, np.NaN])\n",
    "x_nan['bipolarstatus'] = x20126\n",
    "imp.fit(x20126)\n",
    "x20126 = imp.transform(x20126)\n",
    "x20126 = pd.DataFrame(x20126)\n",
    "\n",
    "x_s['bipolarstatus'] = x20126[0]\n",
    "x_s_nan['bipolarstatus'] = x20126[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(x20126[0])\n",
    "x20126 = pd.concat([x20126,dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x20126 = pd.DataFrame(x20126.iloc[:,1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = {0: 'nobipolar',\n",
    "       1: 'bipolarI',      \n",
    "       2: 'bipolarII',\n",
    "       3: 'recurentdepresionsevere',\n",
    "       4: 'recurentdepresionmoderate',\n",
    "       5: 'singledepresionepisode'}\n",
    "x20126 = x20126.rename(columns = var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan_d = pd.concat([x_nan_d, x20126], axis=1)\n",
    "x_s_d = pd.concat([x_s_d, x20126], axis=1)\n",
    "x_s_nan_d = pd.concat([x_s_nan_d, x20126], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes of the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 129)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 129)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 129)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 164)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nan_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 164)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502494, 164)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s_nan_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nan.to_csv(r'x_nan.csv', index = False, header=True)\n",
    "x_s.to_csv(r'x_s.csv', index = False, header=True)\n",
    "x_s_nan.to_csv(r'x_s_nan.csv', index = False, header=True)\n",
    "x_nan_d.to_csv(r'x_nan_d.csv', index = False, header=True)\n",
    "x_s_d.to_csv(r'x_s_d.csv', index = False, header=True)\n",
    "x_s_nan_d.to_csv(r'x_s_nan_d.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f.eid</th>\n",
       "      <th>waistcircum</th>\n",
       "      <th>hipcircum</th>\n",
       "      <th>standingheight</th>\n",
       "      <th>bmi</th>\n",
       "      <th>weight</th>\n",
       "      <th>bodyfatpercent</th>\n",
       "      <th>wholebodyfatfreemass</th>\n",
       "      <th>wholebodywatermass</th>\n",
       "      <th>basalmetabolicrate</th>\n",
       "      <th>...</th>\n",
       "      <th>happiness</th>\n",
       "      <th>everdepressed</th>\n",
       "      <th>perioddepress</th>\n",
       "      <th>numberdepressper</th>\n",
       "      <th>everdesinteresedweek</th>\n",
       "      <th>freqtiredness2weeks</th>\n",
       "      <th>freqtensenness2weeks</th>\n",
       "      <th>frequnenthusiasm2weeks</th>\n",
       "      <th>neuroticismscore</th>\n",
       "      <th>bipolarstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6026563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1000018</td>\n",
       "      <td>103.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>28.6363</td>\n",
       "      <td>95.9</td>\n",
       "      <td>26.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5</td>\n",
       "      <td>8644.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1000020</td>\n",
       "      <td>91.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>27.7698</td>\n",
       "      <td>87.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.9</td>\n",
       "      <td>7837.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000034</td>\n",
       "      <td>93.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>28.7177</td>\n",
       "      <td>76.3</td>\n",
       "      <td>35.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.9</td>\n",
       "      <td>6121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1000041</td>\n",
       "      <td>111.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>28.5572</td>\n",
       "      <td>91.5</td>\n",
       "      <td>22.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.2</td>\n",
       "      <td>8627.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502489</td>\n",
       "      <td>6026519</td>\n",
       "      <td>78.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>24.0725</td>\n",
       "      <td>81.5</td>\n",
       "      <td>35.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.5</td>\n",
       "      <td>6669.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502490</td>\n",
       "      <td>6026524</td>\n",
       "      <td>83.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>23.6728</td>\n",
       "      <td>76.7</td>\n",
       "      <td>31.9</td>\n",
       "      <td>58.6</td>\n",
       "      <td>44.8</td>\n",
       "      <td>7322.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502491</td>\n",
       "      <td>6026535</td>\n",
       "      <td>77.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>22.6736</td>\n",
       "      <td>66.3</td>\n",
       "      <td>36.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.2</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502492</td>\n",
       "      <td>6026540</td>\n",
       "      <td>86.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>29.2050</td>\n",
       "      <td>86.4</td>\n",
       "      <td>21.2</td>\n",
       "      <td>55.9</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6636.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502493</td>\n",
       "      <td>6026551</td>\n",
       "      <td>102.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>35.0116</td>\n",
       "      <td>72.6</td>\n",
       "      <td>15.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.2</td>\n",
       "      <td>4979.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502494 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f.eid  waistcircum  hipcircum  standingheight      bmi  weight  \\\n",
       "0       6026563          NaN        NaN             NaN      NaN     NaN   \n",
       "1       1000018        103.0      106.0           183.0  28.6363    95.9   \n",
       "2       1000020         91.0      109.0           177.0  27.7698    87.0   \n",
       "3       1000034         93.0      102.0           163.0  28.7177    76.3   \n",
       "4       1000041        111.0      108.0           179.0  28.5572    91.5   \n",
       "...         ...          ...        ...             ...      ...     ...   \n",
       "502489  6026519         78.0      106.0           184.0  24.0725    81.5   \n",
       "502490  6026524         83.0      101.0           180.0  23.6728    76.7   \n",
       "502491  6026535         77.0      102.0           171.0  22.6736    66.3   \n",
       "502492  6026540         86.0      111.0           172.0  29.2050    86.4   \n",
       "502493  6026551        102.0      112.0           144.0  35.0116    72.6   \n",
       "\n",
       "        bodyfatpercent  wholebodyfatfreemass  wholebodywatermass  \\\n",
       "0                  NaN                   NaN                 NaN   \n",
       "1                 26.6                   NaN                51.5   \n",
       "2                 26.5                   NaN                46.9   \n",
       "3                 35.7                   NaN                35.9   \n",
       "4                 22.1                   NaN                52.2   \n",
       "...                ...                   ...                 ...   \n",
       "502489            35.5                   NaN                39.5   \n",
       "502490            31.9                  58.6                44.8   \n",
       "502491            36.4                   NaN                35.2   \n",
       "502492            21.2                  55.9                38.0   \n",
       "502493            15.5                   NaN                27.2   \n",
       "\n",
       "        basalmetabolicrate  ...  happiness  everdepressed  perioddepress  \\\n",
       "0                      NaN  ...        NaN            NaN            NaN   \n",
       "1                   8644.0  ...        NaN            NaN            NaN   \n",
       "2                   7837.0  ...        NaN            NaN            NaN   \n",
       "3                   6121.0  ...        NaN            NaN            NaN   \n",
       "4                   8627.0  ...        NaN            NaN            NaN   \n",
       "...                    ...  ...        ...            ...            ...   \n",
       "502489              6669.0  ...        NaN            NaN            NaN   \n",
       "502490              7322.0  ...        3.0            0.0            NaN   \n",
       "502491              5950.0  ...        NaN            NaN            NaN   \n",
       "502492              6636.0  ...        NaN            NaN            NaN   \n",
       "502493              4979.0  ...        NaN            NaN            NaN   \n",
       "\n",
       "        numberdepressper  everdesinteresedweek  freqtiredness2weeks  \\\n",
       "0                    NaN                   NaN                  NaN   \n",
       "1                    NaN                   NaN                  1.0   \n",
       "2                    NaN                   NaN                  NaN   \n",
       "3                    NaN                   NaN                  2.0   \n",
       "4                    NaN                   NaN                  4.0   \n",
       "...                  ...                   ...                  ...   \n",
       "502489               NaN                   NaN                  2.0   \n",
       "502490               NaN                   0.0                  1.0   \n",
       "502491               NaN                   NaN                  1.0   \n",
       "502492               NaN                   NaN                  1.0   \n",
       "502493               NaN                   NaN                  4.0   \n",
       "\n",
       "        freqtensenness2weeks  frequnenthusiasm2weeks  neuroticismscore  \\\n",
       "0                        NaN                     NaN               NaN   \n",
       "1                        1.0                     2.0               NaN   \n",
       "2                        1.0                     1.0               1.0   \n",
       "3                        1.0                     1.0               2.0   \n",
       "4                        1.0                     1.0               0.0   \n",
       "...                      ...                     ...               ...   \n",
       "502489                   2.0                     1.0               5.0   \n",
       "502490                   1.0                     1.0               6.0   \n",
       "502491                   1.0                     1.0               0.0   \n",
       "502492                   NaN                     1.0               7.0   \n",
       "502493                   NaN                     1.0              10.0   \n",
       "\n",
       "        bipolarstatus  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "...               ...  \n",
       "502489            NaN  \n",
       "502490            0.0  \n",
       "502491            NaN  \n",
       "502492            NaN  \n",
       "502493            NaN  \n",
       "\n",
       "[502494 rows x 129 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
